import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):

class MultiHeadAttention(nn.Module):


class EncoderLayer(nn.Module):

class TransformerEncoder(nn.Module):



class LottoTransformer(nn.Module):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward():

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=100):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # 위치 + 주기 기반 sin/cos 인코딩
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # 짝수 인덱스
        pe[:, 1::2] = torch.cos(position * div_term)  # 홀수 인덱스

        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]
        self.register_buffer('pe', pe) # gpu를 따라가는 것

    def forward(self, x):  # x: [batch, seq_len, d_model]
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


if __name__ == "__main__":
    batch_size = 4
    seq_len = 10
    input_dim = 6
    d_model = 32

    # 2-1. Linear Embedding 임베딩 임력 특징 을 차원으로 변환
    embedding_layer = nn.Linear(input_dim, d_model)
    # sample_input = torch.randn(batch_size, seq_len, input_dim)
    sample_input = tensor_from_csv('lotto_recent30.csv', seq_len=10)
    embedded = embedding_layer(sample_input)  # [batch, seq_len, d_model]

    # 2-2. Positional Encoding # 임베딩된 벡터에 위치정보 추가
    pos_encoder = PositionalEncoding(d_model=d_model)
    encoded = pos_encoder(embedded)  # [batch, seq_len, d_model]

    print(encoded.shape)  # 예상: torch.Size([4, 10, 32])